% Do not modify these
\documentclass[fleqn,10pt]{wlscirep}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{sectsty}
\usepackage{float}
\renewcommand{\thesection}{\Roman{section}}
\renewcommand{\thesubsection}{\Alph{subsection}}



% -- Insert any custom LaTeX packages here --

% \package{natbib} % <-- Required for the Chicago citation style
% \package{apacite} % <-- Required for the APA citation style
% If you decide to use one of the styles above, remember to change the \bibliographystyle{} at the bottom of the document too!

\usepackage{listings} % <-- Required if you want to display program source code in your paper.
\usepackage{xcolor}
 
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
% -- End of custom LaTeX packages --
\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour},  
  commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,     
  breaklines=true,         
  captionpos=b,           
  keepspaces=true,         
  numbers=left,         
  numbersep=5pt,          
  showspaces=false,         
  showstringspaces=false,
  showtabs=false,          
  tabsize=2
  }
\usepackage{hyperref}	
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  filecolor=magenta,    
  urlcolor=cyan,
}
\urlstyle{same}

% Fill in your title
\title{Exploring Science Fiction \& Fantasy Q/A}

% Do not modify the author tag below, just let it be blank
\author{}
\twocolumn
% Fill in assignment abstract
\begin{abstract}
In this assignment, we parsed through XML-based datasets using a Hadoop- and Pig Apache. We learned how to write MapReduce jobs and Pig scripts, and how effective it is on our dataset based on XML files from Science Fiction and Fantasy Q/A stack exchange. 

\end{abstract}


% Do not modify the following two lines
\lstset{style=mystyle}
\begin{document}
\include{cover}




% Insert data for the hand-in's cover page
\makecoverpage{
	master_of 	  	 = \par{Applied Computer Science}, % Use either: Applied Computer Science | Human-Computer Interaction
	assignment_title = \par{Exploring Science Fiction and Fantasy}, % Title of your assignment
	course_code   	 = \par{MA140}, % Course code (ex. MA110)
	course_name      = \par{Big Data}, % Course name (ex. Systems Development)
	due_date	    	 = \par{24th Feb 2020}, % Due date
	student_name     = \par{-}, % Your name (or names, if group – separate names with ; semicolon)
	student_number   = \par{865317, 866354}, % Your student ID number (or numbers, if group – separate ID numbers with ; semicolon)
	group_size		   = 2, % Number of group members (used for the declaration text)
}


% Do not modify the following two lines
\flushbottom
\maketitle


% --INTRODUCTION--
\section{Introduction}
This report is of a structure where each task is independent and separately reported, frequently referring to each other due to their similarity. Each contains our assumptions about what the task is asking for, implementation, and any notes/reflections if necessary, opting to add notes/reflection more than necessary rather than less.

We chose to work together because we both have python background, choosing python as the main programming language. We explored a Science Fiction \& Fantasy Q\&A StackExchange dataset based in XML-files as given from \url{archive.org/download/stackexchange/scifi.stackexchange.com.7z} (accessed 17.02.2020) through Hadoop and Pig. 

Hadoop is used to store and process big amounts of data (Bit Data), it has enormous processing power potential and has the ability to handle virtually any number of tasks and jobs. Hadoop provides a framework that allows users to write and test distributed systems and does not rely on hardware to provide fault tolerance, making it less apt for our small-scale applications, yet we use it for the experience. Pig is a MapReduce abstraction, working on Hadoop methods, that we use in some tasks due to its efficiency. We have elected to use both Pig-scripts and pure MapReduce for very similar tasks (see 2c and 2d) to both show our ability to employ both methods and get experience with the two methods.

\section{Main functions}
The datasets consist only XML files which, being of a document-oriented database, is structured into attributes attributed to each element of the file. Being oriented towards use in HTML, i.e. a website, the attributes contain ASCII characters, punctuation, numbers and HTML tags that we don't want to be there. For this, we designed the function cleanBody, meant to clean the text extracted from the XML-documents. Additionally, a function to parse the XML files was required, for which we created xmlparser. Lastly, we created a general function that would perform the duties of putting out the results of the mapper function; mapper\_core. This section explains their implementations, details the modules imported and describes the bash-script used to run our python-scripts: 
\subsection{\emph{cleanBody}}
The cleanBody function formats strings to be parseable by python interpreters. The function removes case sensitivity, ASCII characters, HTML formatting, and treats anything separated by blanked space or / as separated words. This simple implementation of text formatting will affect the results of some tasks. For instance, the name "Jens-Petter" will be interpreted as the word "jenspetter" and the file path /documents/folder/file.extension will be interpreted as the words "documents", "folder" and "fileextension".
The function takes a string as input, formats it and outputs a list of all the words split by the string.split(" ") function.
It uses the sub-function of the open source re module to remove HTML-tags and ASCII function that is inherent starting with in Python 3 to convert all characters to parse-able ASCII text. The symbols in ignore\_char, which are characters we are not considering is defined by the builtin string functions string.punctuation and string.digits.
\begin{lstlisting}[language=Python, caption=cleanBody function]
def cleanBody(body):
  body = body.lower()
  body = ascii(body)
  body = sub("<.+?>","",body)
  body = body.replace("/", " ")
  body = body.strip()

  for i in ignore_char:
    	body = body.replace(i, "")

  body = body.split(" ")

  return body
\end{lstlisting}
\subsection{\emph{Mapper Core}}
mapper\_core is the core of the mapper function; it prints out the relevant data in a format parseable by Hadoop. It functions through three modes, as determined by the parameter mode: "single", "double", and "triple".

\begin{itemize}
 \item Single: Assumes input "words" is a list of words to print. Prints word in words as (word, 1). Ignores empty strings and spaces.
 \item Double: Assumes input "words" is comprised of two nested lists to print. Prints word, count in words as (word, count). Ignores empty strings and spaces.
 \item Triple: Assumes input "words" is comprised of three nested lists to print. Prints id, score, title in words as (id, score, title). Does not ignore empty strings and spaces.
\end{itemize}

\begin{lstlisting}[language=Python, caption=mapper\_core function]
def mapper_core(words, mode="single"):
  if mode == "single":
    for word in words:
      if word not in ["", " "]:
       print("%s %s" %(word,1)) #Emit the word

  elif mode == "double":
    in1, in2 = words
    for word, count in zip(in1,in2):
      if word not in ["", " "]:
        print("%s %s" %(word,count)) #emit the words

  elif mode == "triple":
    in1, in2, in3 = words
    for id, score, title in zip(in1,in2,in3):
      print("%s %s %s" %(id,score, title)) #emit the words

\end{lstlisting}

\subsection{\emph{XML Parser}}
The XML Parser function parses an XML using the open-source xml.etree.ElementTree. It uses sys.stdin as input for normal, Hadoop-based use, but for debugging purposes it also contains a clause that lets it accept a string (see the isinstance(infile, str) check) as a path to the location of an XML-file.\\
Parsing an entire XML-file takes up a significant amount of memory, so we separated it out as an imported function. This makes it simple to exchange it with another method that could more easily deal with larger files. 

\begin{lstlisting}[language=Python, caption=xmlparser function]
def xmlparser(infile):
  if not isinstance(infile, str):
    infile = infile.detach()
  mytree = ET.parse(infile)
  myroot = mytree.getroot()
  return myroot
\end{lstlisting}

\subsection{\emph{Import sys}}
The sys module is an open-source, built in python function that allows us to add to the module search path with sys.path.append(\'../\'), enabling finding ProjectFunctions in a parallel folder, and read in the input for our mapper function and reduce function using sys.stdin.
\begin{lstlisting}[language=Python, caption=Import for sys]
import sys
sys.path.append(\'../\') #allows access functions in parallel folder
import ProjectFunctions.functions as proj
\end{lstlisting}



\subsection{\emph{Import xml.etree.ElementTree}}
ElementTree is a part of the XML open-source library. XML is an inherently hierarchical data format, being document-oriented, and is naturally represented using a tree-like structure. By using the library ElementTree will represent the XML document as a tree, and elements of the tree is represented as single nodes. 
\begin{lstlisting}[language=Python, caption=Import for ElementTree]
import xml.etree.ElementTree as ET
\end{lstlisting}

\subsection{\emph{Import re.sub}}
re.sub is an open-source module we use in cleanBody to remove HTML-tags by removing anything that appears between two chevrons.
\begin{lstlisting}[language=Python, caption=Import for re.sub]
from re import sub
\end{lstlisting}

\subsection{\emph{Bash-script}}
As an example of files being run, this bash-script that was used while testing out scripts is provided. It was used on a Windows PC, meaning scripts had to be converted to Unix code. It automatically names, cleans and saves outputs locally. Below is the base bash-script used for the tasks. The three variables taskNumber, taskName, and sourceFile are used to operate it. The bash-scripts for the tasks containing Pig-scripts have an additional clause for running the Pig-script.
\lstinputlisting[title=Bash-script for a windows-based environment]{Code_output/runBasis_windows.sh}

\subsection{\emph{Data trimming}}
We use the environment Docker, to act as a data cluster. Files that are bigger than 128MB are not readable by the module we use to parse XMLs within the Docker environment (whether this is due to this being the standard size of a data block we were not able to tell). Between the choice of running our software outside Docker and trimming the files for data, we elected to trim the relevant file for this case: Posts.xml. We deleted lines 74 699 through 171 247 and used the remaining data as a basis for our analysis.

% --TASK1--
\section{Task 1 \emph{Warmup}}
%This part of the task is about to get to know how Hadoop- and Pig apache works and how to parse XML files through python code. 
\subsection{\emph{WordCount}}
\textbf{Assumption}: Count the words in body of questions, identified by PostTypeID="1", in \textit{Posts.xml}. The result should be a list of each word and how many times it occurs in the bodies of questions. \\ \\
\textbf{Implementation}: The mapper uses the xmlparser function to parse an XML-file, iterates through all rows in the parsed file, cleans text using the cleanBody function and prints output for the reducer script by using the mapper\_core function.
The reducer receives the mapper output, parses it and iterates through the parsed lines. A counter variable is used to track repeated words, outputting the word with the counter when the word stops repeating. \\ \\
\textbf{Notes/Reflection}: Here the choices made building the cleanBody function affects the output, interpreting some things as words that are not. See line 2 in 1a\_output for example; what is possibly an "accordingly" followed by three line skips and then an "in" is interpreted as "accordinglynnnin" and is counted as a word. \\

\lstinputlisting[firstline=500, lastline=510, title=1a\_output]{Code_output/output1a.txt}

\subsection{\emph{Unique words}}
\textbf{Assumption}: Write a MapReduce job which outputs unique words in the titles of questions, identified by PostTypeID="1", in \textit{Posts.xml}. The result should be a list of each word as it appears in the titles of questions. \\ \\
\textbf{Implementation}: This MapReduce job functions as 1a WordCount does, except it does not count the words and it looks through the titles instead of the bodies. \\ \\
\textbf{Notes/Reflection}: Here again the choices made building the cleanBody function affects the output.
\lstinputlisting[firstline=500, lastline=510, title=1b\_output]{Code_output/output1b.txt}

\subsection{\emph{MoreThan10}}
\textbf{Assumption}: Write a simple python script to check the amount of words in titles of posts in \textit{Posts.xml}. The result should be a count of how many titles have more than 10 words. \\ \\
\textbf{Implementation}: The mapper functions as the mapper in 1a WordCount, except it keeps a counter for the titles of more than 10 words and it outputs the final counter instead of using mapper\_core.
The reducer receives the mapper outputs and sums them up. \\ \\
\textbf{Notes/Reflection}: This MapReduce job is simpler, the reducer is unnecessary outside an environment with clusters.
\lstinputlisting[firstline=1,lastline=1,title=1c\_output]{Code_output/output1c.txt}

\subsection{\emph{Stopwords}}
\textbf{Assumption}: Write a simple python script based on task 1a to exclude \href{https://raw.githubusercontent.com/naimdjon/stopwords/master/stopwords.txt}{stopwords}: from body of questions, identified by PostTypeID="1", in \textit{Posts.xml}. The output should be a word count excluding any stop words. \\ \\
\textbf{Implementation}: This MapReduce job functions as 1a WordCount does, except it has a section for the iterative removal of stopwords, as defined in \href{https://raw.githubusercontent.com/naimdjon/stopwords/master/stopwords.txt}{StopWords.txt}. \\ \\
\textbf{Notes/Reflection}: Given how we built cleanBody, we chose to remove the apostrophes (') from the stopwords, turning words like "we're" to "were".
\lstinputlisting[firstline=500,lastline=509,title=1d\_output]{Code_output/output1d.txt}


\subsection{\emph{Pig top 10}}
\textbf{Assumption}: Write a pig script to select the top 10 listed words after removing the stopwords from \textit{Posts.xml}. The output should be a list of the top 10 listed words and the corresponding occurrence count.\\ \\
\textbf{Implementation}: Given this task being based on 1d StopWords, the mapper and reducer are the same for both tasks. Our pig loaded the output from the MapReduce job, ordered it by count, limited it to 10 elements and stored it into a text file: 1ePig\_Output.txt. \\ \\
\textbf{Notes/Reflection}: We chose for this pig script to load its dataset from an HDFS cluster as opposed to a local storage location.
\lstinputlisting[firstline=1,lastline=10,title=1ePig\_output]{Code_output/output1e.txt}

\subsection{\emph{Tags}}
\textbf{Assumption}: Write a MapReduce job to create a dictionary over unique tags in \textit{Posts.xml}. The result should be a list of unique tags.\\ \\
\textbf{Implementation}: This MapReduce job functions similarly to 1a WordCount, except it has to be aware of whether the post has a tag at all or not, being implemented when tags are extracted from the row. \\ \\
\textbf{Notes/Reflection}: The tags are separated by chevrons and would be fused by cleanBody, so we elected to replace them with spaces. Some of the posts lacked any tag attribute, so we elected to pass over those.
\lstinputlisting[firstline=500,lastline=510,title=1f\_output]{Code_output/output1f.txt}


% --TASK2--
\section{Task 2 \emph{Discover}}
%This part of the task is about to looking through several 

\subsection{\emph{Counting}}
\textbf{Assumption}: Write a MapReduce job to count the total unique users there are in \textit{Users.xml}. The result should be a count of how many unique users there are.\\ \\
\textbf{Implementation}: This MapReduce job functions similarly to 1c MoreThan10, except it counts all unique users instead of titles with more than 10 words. \\ \\
\textbf{Notes/Reflection}: We interpreted "unique user" as a user with a unique Id attribute.
\lstinputlisting[firstline=1,lastline=1,title=2a\_output]{Code_output/output2a.txt}

\subsection{\emph{Unique users}}
\textbf{Assumption}: Write a MapReduce job based on 2a Counting to list the unique users in \textit{Users.xml}. The result should be a list containing unique users in the dataset. \\ \\
\textbf{Implementation}: This MapReduce job functions as 2a Counting does, except it outputs all unique users instead of a count of them \\ \\
\textbf{Notes/Reflection}: Here the implementation of cleanBody is modified with a join function as cleanBody returns a list where we want a string. Since usernames can contain spaces, we elected to instead use | as a separator in hadoop.
 \lstinputlisting[firstline=500,lastline=510,title=2b\_output]{Code_output/output2b.txt}

\subsection{\emph{Top users}}
\textbf{Assumption}: Write a pig script or MapReduce job to find the top 10 users based on the Reputation attribute in \textit{Users.xml}. The result should be a list of the 10 users based on the highest reputation value. \\ \\
\textbf{Implementation}: This mapper job is based on 2b Unique users, except instead of putting out a name and its corresponding id, it puts out a name and its corresponding reputation attribute. Here, reputation is given to mapper\_core as a one-element list (mapper\_core called with arguments (name, [reputation])) due to its looping through its inputs. The reducer is the same as the reducer of 2b Unique users. The pig script functions the same way as in 1e Pig Top 10\\ \\
\textbf{Notes/Reflection}: We elected here to use Pig-script to order the MapReduce output to get experience.
\lstinputlisting[firstline=1,lastline=10,title=2c\_output]{Code_output/output2c.txt}

\subsection{\emph{Top questions}}
\textbf{Assumption}: Write a pig script or MapReduce job to find top 10 title questions, identified by PostTypeID="1", in \textit{Posts.xml} based on the Score attribute. The result should be a list containing the top 10 questions, showing id, question, and score.\\ \\
\textbf{Implementation}: This mapper job functions similarly to 2c Top users except it puts out 3 parsed attributes using the "triple"-mode of mapper\_core. The reducer performs the reduction and ordering, finding the 10 highest score values. It does this by inserting every input into a list instead of printing it, using pythons standard function string.sort to order it. It then iterates through the 10 highest values to give an output.\\ \\
\textbf{Notes/Reflection}: The reducer could potentially be improved by only keeping 10 elements in the list at once, changing them out as higher scores were encountered.
\lstinputlisting[firstline=1,lastline=10,title=2d\_output]{Code_output/output2d.txt}

\subsection{\emph{Favorite questions}}
\textbf{Assumption}: Write a pig script or MapReduce job to find top 10 title questions, identified by PostTypeID="1", in \textit{Posts.xml} based on FavouriteCount attribute. The result should be a list containing the top 10 questions, showing id, question and the favourite count. \\ \\
\textbf{Implementation}: This MapReduce job is the same as 2d Top questions, except it extracts the FavoriteCount-attribute instead of the score \\ \\
\textbf{Notes/Reflection}: Here, like 1f Tags, some posts had to be skipped due to their lacking any FavoriteCount-attribute.
\lstinputlisting[firstline=1,lastline=10,title=2e\_output]{Code_output/output2e.txt}

\subsection{\emph{Average answers}}
\textbf{Assumption}: Write a MapReduce job that calculates the average number of answers per question. The result should be a calculated average of all AnswerCount attributes. \\ \\
\textbf{Implementation}: This mapper functions similarly to 1c MoreThan10, except for having two counters, one for score and the other for users parsed, which are averaged at the end. The reducer is the same as the reducer in 2a Counting \\ \\
\textbf{Notes/Reflection}: We have elected to not limit the amount of decimals.
\lstinputlisting[firstline=1,lastline=10,title=2f\_output]{Code_output/output2f.txt}

\subsection{\emph{Countries}}
\textbf{Assumption}: Write a MapReduce job to count the amount of users attributed to a country by the Location attribute in \textit{Users.xml}. The result should be a list of different countries and corresponding users. \\ \\
\textbf{Implementation}: This mapper functions similarly to 1f tags, except instead of cleaning the tags, it uses "|" as a separator since spaces are part of the location. If Location is separated by a \',\' the last part is treated as the country. \\ \\
\textbf{Notes/Reflection}: Because by our judgment locations are typed in by users, we have elected to separate the country out of Location very simply by using \',\' as a separator. Anything after the last \',\' is treated as a country. A location with no \',\' is treated as a country. Locations like "California, USA", "California, United States", and "US" will be interpreted as "usa", "united states", and "us", respectively.
\lstinputlisting[firstline=1,lastline=10,title=2g\_output]{Code_output/output2g.txt}

\subsection{\emph{Names}}
\textbf{Assumption}: Write a pig script or MapReduce job to find the 10 most popular names in \textit{Users.xml}. The result should be a list of top 10 common names. \\ \\
\textbf{Implementation}: This mapper functions similarly to 2g Countries, except it extracts the DisplayName-attribute in place of the Location-attribute. The reducer functions similarly to 2d Top questions, except it parses differently and has different text formatting for the output due to the decrease from 3 to 2 arguments \\ \\
\textbf{Notes/Reflection}: Here the most common name is User. This is due to our choice of removing numbers in cleanBody, combining User1, User2, User3, etc. The task suggested explicitly that we should split up names by spaces, perhaps wanting to separate out last names from first names. Due to the nature of Displaynames usually not being full legal names, we have elected to interpret a "name" as the full name given in DisplayName.
\lstinputlisting[firstline=1,lastline=21,title=2h\_output]{Code_output/output2h.txt}


\subsection{\emph{Answers}}
\textbf{Assumption}: Write a python script to find how many questions, identified by PostTypeID="1", in \textit{Posts.xml} have at least one answer based on attribute AnswerCount. The result should be a count of how many questions have been answered. \\ \\
\textbf{Implementation}: This mapper functions in the same way as 1c MoreThan10 except it checks whether AnswerCount is larger than or equal to 1 instead of whether the title word length is larger than 10.
\lstinputlisting[firstline=1,lastline=10,title=2i\_output]{Code_output/output2i.txt}

% --TASK3--
\section{Task 3 \emph{Numbers}}

\subsection{\emph{Bigram}}
\textbf{Assumption}: We chose to write a MapReduce job to find the most common pair of adjacent words in the titles of questions, identified by PostTypeID="1", in \textit{Posts.xml}. A bigram is a combination of two words in a sequence. For instance the sentence "Big data is big" contains the bigrams "Big data", "data is", and "is big". The result should be the most common bigram along with a count of how many times it occurs. \\ \\
\textbf{Implementation}: This MapReduce is the same as 1a WordCount, except we output each bigram in the body instead of each word. Because of this we he had to forego the mapper\_core for a print function inside a loop looping through every bigram. The reducer finds the most common bigram by iteratively comparing each elements second element (the bigrams count) against a variable declared maximum. \\ \\
\textbf{Notes/Reflection}: In the print-loop, we had to limit the loop to i in range(len(words)-1), since we printed both words[i] and words[i+1], and doing otherwise would result in and out of range error.
\lstinputlisting[firstline=1,lastline=10,title=3a\_output]{Code_output/output3a.txt}

\subsection{\emph{Trigram}}
\textbf{Assumption}: Perform the same task as in 3a Bigram, except operate with trigrams (sequences of 3 words as opposed to 2). The result should be the most common trigram. \\ \\
\textbf{Implementation}: This is exactly the same as 3a Bigram, except we output trigrams instead of bigrams. \\ \\
\textbf{Notes/Reflection}: The mapper, as in 3a Bigram, limited the print-loop to range(len(words)-2) and printed words[i], words[i+1], and words[i+2].
\lstinputlisting[firstline=1,lastline=10,title=3b\_output]{Code_output/output3b.txt}

\subsection{\emph{Combiner}}
\textbf{Assumption}: Perform the same task as in 1a WordCount, except add a combiner before the mapper sends information to the reducer. The result should be the same as in 1a WordCount, but the volume of the data transfer should be smaller. \\ \\
\textbf{Implementation} The MapReduce function is the same as in 1a WordCount, with a combiner ahead of the print function in the mapper. \\ \\
\textbf{Notes/Reflection}: Having a combiner in a MapReduce job saves bandwidth and computational strain by decreasing the volume of data sent from the mapper, as shown in \ref{fig:figure_label}.
\lstinputlisting[firstline=1,lastline=10,title=3c\_output]{Code_output/output3c.txt}

\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{images/time.png}
\caption{TIming for our two methods, Leftmost shows 3c combiner, rightmost shows 1a WordCount}
\label{fig:figure_label}
\end{figure}

\subsection{\emph{Useless}}
\textbf{Assumption}: Write a MapReduce job to find how many times the word "useless" in occurs in the bodies of questions, identified by PostTypeID="1", in \textit{Posts.xml}. The result should be a count of how many times the word useless occurs. \\ \\
\textbf{Implementation}: This mapper functions similarly to 1c MoreThan10 and 2i Answers, except instead of checking for titles with more words than 10 or whether AnswerCount is larger than or equal to 1, it counts whether the word "useless" occurs in the body. \\ \\
\lstinputlisting[firstline=1,lastline=10,title=3d\_output]{Code_output/output3d.txt}


% --TASK4--
\section{Task 4 \emph{Search engine}}
\subsection{\emph{Title index}}
\textbf{Assumption}: Write MapReduce job to create an index over titles, bodies, and answers of questions in \textit{Posts.xml}. The result should be a simple index that lists posts words appear in by their Id's. \\ \\
\textbf{Implementation}: This mapper function parses through all rows in posts.xml. It then forks over whether PostTypeId is 1 or 2, i.e. whether it is an answer or question, extracting the relevant id, body, and title from the row. It contains a combiner to speed up the process, removing duplicates. The reducer parses the input and iterates through all the lines. It uses two variables to know what the last word and associated Id was, skipping the word if both are the same, adding the Id to a list if \textit{only} the word is the same, and putting out the word with an associated list of Id's if neither is the same, i.e. it has finished working with the word. \\ \\
\textbf{Notes/Reflection}: We elected to have a combiner in the mapper although it is not strictly necessary since we are working with small file-sizes.
\lstinputlisting[firstline=1792,lastline=1800, title=4a\_output]{Code_output/output4.txt}



\section{Conclusion}
Through this assignment, we gained experience in using MapReduce jobs and pig scripts. We parsed XML-documents and extracted specific information we were looking for to be formatted to an output for each task.



% Do not modify this last lines
\end{document}

































